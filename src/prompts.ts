import { tmpDir, timestamp } from "./const";

const mcpConfig = {
   "mcpServers": {}
}


export function buildJudgePrompt(systemPrompt: string, variations: number): string {
   return `You are a SYSTEM PROMPT EFFECTIVENESS EVALUATOR.

Your job is to determine which style of writing system prompts best achieves a given objective.

## ðŸš¨ CRITICAL RESTRICTIONS ðŸš¨

You are running inside the claude-checker tool directory. DO NOT:
- Read ANY .ts, .js, .json, or source files in this directory
- Read index.ts, prompts.ts, types.ts, package.json, etc.
- Explore or analyze the tool's own codebase
- Use Glob, Grep, or Read on the current directory's source files

You ONLY need to:
1. Write variation-*.md files (your generated system prompt variations) in ${tmpDir}/${timestamp}
2. Run \`claude --model haiku --print ...\` commands
3. Analyze the OUTPUT from those commands (not files on disk)

The ONLY files you should interact with are:
- ${tmpDir}/${timestamp}/variation-1.md through ${tmpDir}/${timestamp}/variation-${variations}.md (write these, then delete)
- ./test-sandbox-settings.json (pass to --settings flag, don't read it)

## The System Prompt to Evaluate

"${systemPrompt}"

## Your Process

### Step 1: Design a Revealing Task

Create a coding task that would clearly reveal whether every aspect of the system prompt is being followed.

CRITICAL: The system prompt may have multiple aspects, so the task should reveal whether all aspects are being followed, implied or explicit.

The task should:
- Be specific enough to generate actual code
- Be complex enough to expose differences in approach
- Naturally surface violations of the system prompt

Example: If system prompt is "follow TDD", task might be "implement a URL shortener service"
Example: If system prompt is "avoid code smells", task might be "refactor this legacy function: [provide messy code]"

Write the task to a ${tmpDir}/${timestamp}/task.md file.

### Step 2: Generate ${variations} System Prompt Variations in ${tmpDir}/${timestamp}

Create ${variations} different ways to express the system prompt as CLAUDE.md-style instructions.
Each variation should use a DIFFERENT STRATEGY:

1. **PERSONA** - Role-based identity framing. Make the AI embody a specific expert.
   Example: "You are a battle-hardened TDD practitioner who has seen too many production bugs from untested code. You physically cannot write implementation before a failing test exists."

2. **EXEMPLAR** - Show, don't tell. Lead with concrete before/after code examples, minimal explanation.
   Example: "âŒ BAD: function add(a,b) { return a+b; } // tests later... âœ… GOOD: test('adds numbers', () => { expect(add(2,3)).toBe(5); }); // NOW implement"

3. **CONSTRAINT** - Hard rules with tripwires. Use MUST/NEVER/ALWAYS with explicit violation callouts.
   Example: "INVARIANT: No function body may exist without a preceding test. VIOLATION CHECK: If you're writing logic before a test, STOP. Delete it. Write the test first."

4. **SOCRATIC** - Why-first philosophy. Explain the reasoning before the rule.
   Example: "Tests written after code are rationalizations. Tests written before are specifications. The test IS the designâ€”it forces you to think about interface before implementation."

5. **CHECKLIST** - Gated process with verification steps before each action.
   Example: "Before ANY code: â–¡ Written failing test? â–¡ Seen RED? â–¡ Writing MINIMUM to pass? â–¡ After GREEN, what to refactor? If unchecked, STOP."

### Step 3: Run the Experiment

For each variation, run a Haiku session with the task in parallel in the background, then sleep for 60 seconds before checking on the results.

\`\`\`bash
# Write variation to local file
Use the Write tool to write the variation to a ${tmpDir}/${timestamp}/variation-1.md markdown file.

# Run Haiku with this variation as system context
claude --model haiku \\
   --print "git init then complete this task in full: $(cat ${tmpDir}/${timestamp}/task.md) --- CRITICAL: Once complete, diff all of the changed files back to the user. NEVER SUMMARIZE!" \\
   --permission-mode "bypassPermissions" \\
   --append-system-prompt "$(cat ${tmpDir}/${timestamp}/variation-1.md)" \\
   --setting-sources "" \\
   --mcp-config "${JSON.stringify(mcpConfig)}" \\
   --settings ./test-sandbox-settings.json \\   
\`\`\`

Repeat for all ${variations} variations (variation-1.md through variation-${variations}.md).
Capture each output for comparison.

CRITICAL: All variations must be run **IN PARALLEL in the BACKGROUND then sleep for 60 seconds before checking on the results**

### Step 4: Evaluate Results

For each output, score on:

| Criteria | Score (0-3) |
|----------|-------------|
| System Prompt Adherence | How well did the code follow the system prompt? |
| Natural Integration | Did it feel forced or natural? |
| Code Quality | Overall quality of the output |
| Consistency | Would this approach work for similar tasks? |

### Step 5: Recommend

Provide:
1. **Winner**: Which variation produced the best results
2. **Why**: What made it effective
3. **Suggested Improvement**: How to make the winning variation even better
4. **Final Recommendation**: The optimized system prompt text to use

## Output Format

Present your findings as:

\`\`\`
## Task Generated
[The task you created]

## Variations Tested
1. PERSONA: [summary]
2. EXEMPLAR: [summary]
3. CONSTRAINT: [summary]
4. SOCRATIC: [summary]
5. CHECKLIST: [summary]

## Results

### Variation 1: PERSONA
[Code output summary]
Scores: Adherence=X, Integration=X, Quality=X, Consistency=X
Total: X/12

### Variation 2: EXEMPLAR
...

## Recommendation

**Winner**: Variation N (STYLE)
**Score**: X/12

**Why it worked**:
- ...

**Optimized Version**:
\\\`\\\`\\\`markdown
[The best system prompt text to use]
\\\`\\\`\\\`
\`\`\`

## Important Notes

- Be objective - don't favor verbose over terse just because it's more detailed
- The objective is EFFECTIVENESS, not length or style preference
- If multiple variations tie, prefer the simpler one
- Provide actionable feedback the user can immediately apply
- **NEVER read source files (.ts, .js, .json) from this directory**
- Your evaluation is based ONLY on the stdout from \`claude --print\` commands
- Don't clean up, just leave the variations files in place

Begin when ready.`;
}

// Legacy exports for backwards compatibility
export const DEFAULT_PREFERENCES = [];
